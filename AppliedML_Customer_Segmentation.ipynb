{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of SIT_W2D1_HT1_Customer_Segmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmAchieng/DataSciencePracticeSeries/blob/master/AppliedML_Customer_Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of3HGFCW2ii7"
      },
      "source": [
        "<a id='Q0'></a>\n",
        "<center><a target=\"_blank\" href=\"http://www.propulsion.academy\"><img src=\"https://drive.google.com/uc?id=1McNxpNrSwfqu1w-QtlOmPSmfULvkkMQV\" width=\"200\" style=\"background:none; border:none; box-shadow:none;\" /></a> </center>\n",
        "<center> <h4 style=\"color:#303030\"> Introduction to Data Science, Week 2 Day 1, Tutorial 1: </h4> </center>\n",
        "<center> <h1 style=\"color:#303030\">Customer Segmentation</h1> </center>\n",
        "<p style=\"margin-bottom:1cm;\"></p>\n",
        "<center style=\"color:#303030\"><h4>Propulsion Academy, 2021</h4></center>\n",
        "<p style=\"margin-bottom:1cm;\"></p>\n",
        "\n",
        "<div style=\"background:#EEEDF5;border-top:0.1cm solid #EF475B;border-bottom:0.1cm solid #EF475B;\">\n",
        "    <div style=\"margin-left: 0.5cm;margin-top: 0.5cm;margin-bottom: 0.5cm\">\n",
        "        <p><strong>Goal:</strong> Practice Unsupervised learning on the data set of an online retail store</p>\n",
        "        <strong> Sections:</strong>\n",
        "        <a id=\"P0\" name=\"P0\"></a>\n",
        "        <ol>\n",
        "            <li> <a style=\"color:#303030\" href=\"#SU\">Set Up </a> </li>\n",
        "            <li> <a style=\"color:#303030\" href=\"#P1\">Exploratory Data Analysis</a></li>\n",
        "            <li> <a style=\"color:#303030\" href=\"#P2\">Modeling</a></li>\n",
        "        </ol>\n",
        "        <strong>Topics Trained:</strong> Clustering.\n",
        "    </div>\n",
        "</div>\n",
        "\n",
        "\n",
        "<nav style=\"text-align:right\"><strong>\n",
        "        <a style=\"color:#00BAE5\" href=\"https://monolith.propulsion-home.ch/backend/api/momentum/materials/intro-2-ds-materials/\" title=\"momentum\"> SIT Introduction to Data Science</a>|\n",
        "        <a style=\"color:#00BAE5\" href=\"https://monolith.propulsion-home.ch/backend/api/momentum/materials/intro-2-ds-materials/weeks/week2/day1/index.html\" title=\"momentum\">Week 2 Day 1, Applied Machine Learning</a>|\n",
        "        <a style=\"color:#00BAE5\" href=\"https://colab.research.google.com/drive/13CHiIQTM2yFb0W6b1AO8OF3Oy3xDeIAD?usp=sharing\" title=\"momentum\"> Tutorial 1, Customer Segmentation</a>\n",
        "</strong></nav>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckLGGhLpmYD8"
      },
      "source": [
        "<a id='SU' name=\"SU\"></a>\n",
        "## [Set up](#P0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTC_OuUpBXlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9e73743-f3d0-4306-cc7e-f2bae310449b"
      },
      "source": [
        "!pip install pandas-profiling==2.7.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pandas-profiling==2.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/8a/25da481171f4912e2515a76fe31b7a4f036a443b8858b244ef7daaffd5b6/pandas_profiling-2.7.1-py2.py3-none-any.whl (252kB)\n",
            "\r\u001b[K     |█▎                              | 10kB 14.1MB/s eta 0:00:01\r\u001b[K     |██▋                             | 20kB 19.2MB/s eta 0:00:01\r\u001b[K     |████                            | 30kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 40kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 51kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 61kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 71kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 81kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 92kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 102kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 112kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 122kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 133kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 143kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 153kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 163kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 174kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 184kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 194kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 204kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 215kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 225kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 235kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 245kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 256kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipywidgets>=7.5.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==2.7.1) (7.6.3)\n",
            "Requirement already satisfied: astropy>=4.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==2.7.1) (4.2)\n",
            "Collecting htmlmin>=0.1.12\n",
            "  Downloading https://files.pythonhosted.org/packages/b3/e7/fcd59e12169de19f0131ff2812077f964c6b960e7c09804d30a7bf2ab461/htmlmin-0.1.12.tar.gz\n",
            "Collecting tangled-up-in-unicode>=0.0.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/d1/58bfbe263494741a47140049b989ad42a8941854e8d34f1af90640c6c9f9/tangled_up_in_unicode-0.0.7-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 12.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: missingno>=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==2.7.1) (0.4.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==2.7.1) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==2.7.1) (2.23.0)\n",
            "Collecting tqdm>=4.43.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/8a/34efae5cf9924328a8f34eeb2fdaae14c011462d9f0e3fcded48e1266d1c/tqdm-4.60.0-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.6MB/s \n",
            "\u001b[?25hCollecting phik>=0.9.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/ce/193e8ddf62d4be643b9b4b20e8e9c63b2f6a20f92778c0410c629f89bdaa/phik-0.11.2.tar.gz (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 41.0MB/s \n",
            "\u001b[?25hCollecting confuse>=1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/6d/55/b4726d81e5d6509fa3441f770f8a9524612627dc1b2a7d6209d1d20083fe/confuse-1.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==2.7.1) (3.2.2)\n",
            "Requirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==2.7.1) (2.11.3)\n",
            "Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==2.7.1) (1.1.5)\n",
            "Collecting visions[type_image_path]==0.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/fe/7614dec3db3f20882ff12dae0a58b579e97b590f2994ce9c953fe179d512/visions-0.4.1-py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==2.7.1) (1.19.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==2.7.1) (1.0.1)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->pandas-profiling==2.7.1) (5.0.5)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->pandas-profiling==2.7.1) (3.5.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->pandas-profiling==2.7.1) (1.0.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->pandas-profiling==2.7.1) (4.10.1)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->pandas-profiling==2.7.1) (5.5.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->pandas-profiling==2.7.1) (5.1.2)\n",
            "Requirement already satisfied: pyerfa in /usr/local/lib/python3.7/dist-packages (from astropy>=4.0->pandas-profiling==2.7.1) (1.7.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from missingno>=0.4.2->pandas-profiling==2.7.1) (0.11.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->pandas-profiling==2.7.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->pandas-profiling==2.7.1) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->pandas-profiling==2.7.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->pandas-profiling==2.7.1) (1.24.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from confuse>=1.0.0->pandas-profiling==2.7.1) (3.13)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.7.1) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.7.1) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.7.1) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.7.1) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.11.1->pandas-profiling==2.7.1) (1.1.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3->pandas-profiling==2.7.1) (2018.9)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.4.1->pandas-profiling==2.7.1) (20.3.0)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.4.1->pandas-profiling==2.7.1) (2.5)\n",
            "Collecting imagehash; extra == \"type_image_path\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/18/9dbb772b5ef73a3069c66bb5bf29b9fb4dd57af0d5790c781c3f559bcca6/ImageHash-4.2.0-py2.py3-none-any.whl (295kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 30.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow; extra == \"type_image_path\" in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.4.1->pandas-profiling==2.7.1) (7.1.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.3.1->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.2.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (5.3.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (54.2.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (1.0.18)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (4.7.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (2.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=3.2.0->pandas-profiling==2.7.1) (1.15.0)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash; extra == \"type_image_path\"->visions[type_image_path]==0.4.1->pandas-profiling==2.7.1) (1.1.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.9.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (1.5.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (22.0.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.2.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (3.3.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (1.4.3)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.4.4)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.8.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (20.9)\n",
            "Building wheels for collected packages: htmlmin, phik\n",
            "  Building wheel for htmlmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for htmlmin: filename=htmlmin-0.1.12-cp37-none-any.whl size=27085 sha256=9fa95cf88be8c72ee1043ea909dd029c32e91b3a8158c011e423b72d286c187b\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/07/ac/7c5a9d708d65247ac1f94066cf1db075540b85716c30255459\n",
            "  Building wheel for phik (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for phik: filename=phik-0.11.2-cp37-none-any.whl size=1107413 sha256=023f45f8246921a1c8e7f403b9670dc56c2367d9d5d6e872fc9807368ecda128\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/a3/b0/f27b1cfe32ea131a3715169132ff6d85653789e80e966c3bf6\n",
            "Successfully built htmlmin phik\n",
            "\u001b[31mERROR: phik 0.11.2 has requirement scipy>=1.5.2, but you'll have scipy 1.4.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: htmlmin, tangled-up-in-unicode, tqdm, phik, confuse, imagehash, visions, pandas-profiling\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: pandas-profiling 1.4.1\n",
            "    Uninstalling pandas-profiling-1.4.1:\n",
            "      Successfully uninstalled pandas-profiling-1.4.1\n",
            "Successfully installed confuse-1.4.0 htmlmin-0.1.12 imagehash-4.2.0 pandas-profiling-2.7.1 phik-0.11.2 tangled-up-in-unicode-0.0.7 tqdm-4.60.0 visions-0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQF1r4IzM0hE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9039fda9-ac2a-45d0-b167-d8269e5d656e"
      },
      "source": [
        "!pip install --upgrade plotly"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting plotly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/f6/bd3c17c8003b6641df1228e80e1acac97ed8402635e46c2571f8e1ef63af/plotly-4.14.3-py2.py3-none-any.whl (13.2MB)\n",
            "\u001b[K     |████████████████████████████████| 13.2MB 303kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly) (1.3.3)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from plotly) (1.15.0)\n",
            "Installing collected packages: plotly\n",
            "  Found existing installation: plotly 4.4.1\n",
            "    Uninstalling plotly-4.4.1:\n",
            "      Successfully uninstalled plotly-4.4.1\n",
            "Successfully installed plotly-4.14.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWkVZjBMgayP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fdbcd03-3a16-4cfb-ce6c-6d7424808bec"
      },
      "source": [
        "!pip3 install -U scikit-learn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/74/eb899f41d55f957e2591cde5528e75871f817d9fb46d4732423ecaca736d/scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 121kB/s \n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.1 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5taGRcD1NG86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c250f4c-4451-49e9-ca33-4b009d3322e4"
      },
      "source": [
        "!pip install -U kaleido"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kaleido\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/b3/a0f0f4faac229b0011d8c4a7ee6da7c2dca0b6fd08039c95920846f23ca4/kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9MB)\n",
            "\u001b[K     |████████████████████████████████| 79.9MB 82kB/s \n",
            "\u001b[?25hInstalling collected packages: kaleido\n",
            "Successfully installed kaleido-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJG1QFVYGzHX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8995324e-1fd9-494e-fbaf-518d1fb42f10"
      },
      "source": [
        "!pip install -U yellowbrick"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting yellowbrick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/15/58feb940b6a2f52d3335cccf9e5d00704ec5ba62782da83f7e2abeca5e4b/yellowbrick-1.3.post1-py3-none-any.whl (271kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 16.0MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 21.1MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30kB 18.8MB/s eta 0:00:01\r\u001b[K     |████▉                           | 40kB 12.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 61kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 71kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 81kB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 92kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 102kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 112kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 122kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 133kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 143kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 153kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 163kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 174kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 184kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 194kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 204kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 215kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 225kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 235kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 245kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 256kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 266kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy<1.20,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from yellowbrick) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from yellowbrick) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from yellowbrick) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from yellowbrick) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from yellowbrick) (0.24.1)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10.0->yellowbrick) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->yellowbrick) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->yellowbrick) (2.1.0)\n",
            "Installing collected packages: yellowbrick\n",
            "  Found existing installation: yellowbrick 0.9.1\n",
            "    Uninstalling yellowbrick-0.9.1:\n",
            "      Successfully uninstalled yellowbrick-0.9.1\n",
            "Successfully installed yellowbrick-1.3.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUbVHNTOb5mj"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from pandas_profiling import ProfileReport\n",
        "#import pandas_profiling\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly\n",
        "plotly.__version__\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmyLj8_RG6q3"
      },
      "source": [
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "from yellowbrick.cluster import SilhouetteVisualizer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-HBcRDHM9cD"
      },
      "source": [
        "from sklearn.impute import SimpleImputer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4otsJv3U2GH"
      },
      "source": [
        "new packages for this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKn4L0U4KwHu"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn import set_config\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9B-eWIFB579h"
      },
      "source": [
        "Connect to your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAo_4yUyT_H_"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4iSfOb_alvK"
      },
      "source": [
        "data_path = \"/content/drive/MyDrive/Introduction2DataScience/data/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aI9XNuS370W2"
      },
      "source": [
        "pd.set_option('display.max_rows', 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftzjD7zigOqF"
      },
      "source": [
        "set_config(display='diagram')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO6--w_l6nKZ"
      },
      "source": [
        "The dataset for this exercise is  downloadable [here](https://drive.google.com/file/d/1oKZkv2oakO2nasl26MLd2SDsgu6dEVzW/view?usp=sharing). Add the file to your google Drive, within the folder Introduction2DataScience/data/ that you created for the course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6UK6Rj4ozP4"
      },
      "source": [
        "<a id='P1' name=\"P1\"></a>\n",
        "## [Exploratory Data Analysis](#P0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uLNlAUdLS7G"
      },
      "source": [
        "### Understand the Context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBLOtZYzhqcO"
      },
      "source": [
        "**What type of problem are we trying to solve?**\n",
        "\n",
        "With this data set, we want to explore the customer base of an online retail shop. The end goal would be to answer the following question:\n",
        "\n",
        "**_Can we segment our Customer base in different meaningful groups?_**\n",
        "\n",
        "This would allow us to design different marketing campaigns, targeted for each group, for example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOGOouUFLX82"
      },
      "source": [
        "**How was the data collected?/ Is there documentation on the Data?**\n",
        "\n",
        "This is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n",
        "\n",
        "- **InvoiceNo:** Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.\n",
        "- **StockCode:** Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n",
        "- **Description:** Product (item) name. Nominal.\n",
        "- **Quantity:** The quantities of each product (item) per transaction. Numeric.\n",
        "- **InvoiceDate:** Invoice Date and time. Numeric, the day and time when each transaction was generated.\n",
        "- **UnitPrice:** Unit price. Numeric, Product price per unit in sterling.\n",
        "- **CustomerID:** Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n",
        "- **Country:** Country name. Nominal, the name of the country where each customer resides.\n",
        "\n",
        "**Has there already been Machine Learning projects on this data?**\n",
        "\n",
        "this dataset is made available by [UCI](http://archive.ics.uci.edu/ml/datasets/online+retail) and has been donated by Dr Daqing Chen, Director: Public Analytics group, School of Engineering, London South Bank University, London SE1 0AA, UK.\n",
        "\n",
        "It is linked to the following article:\n",
        "[Daqing Chen, Sai Liang Sain, and Kun Guo, Data mining for the online retail industry: A case study of RFM model-based customer segmentation using data mining, Journal of Database Marketing and Customer Strategy Management, Vol. 19, No. 3, pp. 197-208, 2012](https://openresearch.lsbu.ac.uk/download/a7d91f552531769d0e73e3add16f49ee8f0e357e79c994f202314fbc779cb301/356215/dbm201217a%20Data%20mining%20for%20the%20online%20retail%20industry%20-%20A%20case%20study%20of%20RFM%20model-based%20customer%20segmentation%20using%20data%20mining.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5ntlub0gnOS"
      },
      "source": [
        "**Do we have assumption about the data?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yACAESBhgtPu"
      },
      "source": [
        "From the description of the columns, we could assume that StockCode and Description roughly contain the same information, although 2 products could happen to have exactly the same name."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX4mRdMggw66"
      },
      "source": [
        "**Can we foresee any challenge related to this data set?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dXDi_Z3g0pL"
      },
      "source": [
        "- There is a time component, so we should take care of how we split our train and test sets.\n",
        "\n",
        "- We have a limited dataset of less than a year of sales. there might be some seasonal effects that we cannot fully study.\n",
        "\n",
        "- some lines correspond to cancellations, so we might have to treat them separately.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xySiqwWb9PGD"
      },
      "source": [
        "### Data Structure and types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOJxFTUz9T96"
      },
      "source": [
        "**Load the csv file as a DataFrame using Pandas**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmQJATbt8EsE"
      },
      "source": [
        "online_retail = pd.read_csv(f'{data_path}OnlineRetail.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQzf4vdT9hAR"
      },
      "source": [
        "**How many columns and rows do we have?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYbAYJKa9uSz"
      },
      "source": [
        "online_retail.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbhbgHQT9zfj"
      },
      "source": [
        "**What are the names and meaning of each columns?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR5xJbyv94NF"
      },
      "source": [
        "online_retail.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqvsSmec-R2l"
      },
      "source": [
        "print the first 10 rows of the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb_OymLN-lNy"
      },
      "source": [
        "online_retail.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tetG1lEj98OY"
      },
      "source": [
        "**What are the types of each column?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPLU-32q-NC9"
      },
      "source": [
        "online_retail.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNebgovg-VvK"
      },
      "source": [
        "**Do the types correspond to what you expected?\n",
        "if not, which columns would you change and why?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AKxj42R-wxG"
      },
      "source": [
        "<!--- BEGIN SOLUTION --->\n",
        "- Invoice date should be of type datetime\n",
        "- the customer ID should be of type object\n",
        "<!--- END SOLUTION --->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJeEM7R8_M1-"
      },
      "source": [
        "**Perform the necessary type transformations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "id5_rx6S_Snn"
      },
      "source": [
        "online_retail['InvoiceDate'] = pd.to_datetime(online_retail['InvoiceDate'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EOjPT0qBUmU"
      },
      "source": [
        "online_retail['CustomerID'] = online_retail['CustomerID'].astype(object)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aecX46HSM3lA"
      },
      "source": [
        "**What are the possible categories for categorical columns?/What is the min, max and mean of each numerical columns?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddVJHW8DsDEZ"
      },
      "source": [
        "Let's answer these questions by using the describe method on all columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAaNmcw99iyi"
      },
      "source": [
        "online_retail.describe(include='all',datetime_is_numeric=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFbpwRJZsRh7"
      },
      "source": [
        "InvoiceNo, Stockcode, Description, CustomerID and Country take discrete values. However the number of unique values are of a few thousands for all these, so it does not make sense to print them. \n",
        "\n",
        "let's just have a look at the Country column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peqO8YDWsSHD"
      },
      "source": [
        "online_retail['Country'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RvWIR-MuHWC"
      },
      "source": [
        "here, we see that we have an \"unspecified\" category, which amounts to missing data. We see as well that there is the common \"European Community\" category, but also categories for some specific countries belonging to the European Community, like France. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxJai1EjM0AT"
      },
      "source": [
        "**Perform test/train split here**\n",
        "\n",
        "!!! Please think about it!!! How should the data be splitted?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rMDWYejyd6f"
      },
      "source": [
        "Now this is a tricky topic for this data set, and we have to think a bit more about what we want to accomplish. \n",
        "\n",
        "First, we have a time-bound dataset. However, the **problem** we want to solve is not to predict a variable in the future, it is to answer this question:\n",
        "\n",
        "**_Can we segment our Customer base in different meaningful groups?_**\n",
        "\n",
        "As we saw before, the table that we have is not sorted by customer, but by transaction. There could be several lines refering to the same customer, so we cannot randomly split the rows of our table to get a train and a test set!\n",
        "\n",
        "If we want to make a rigorous split of the data, we should split by customer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iavcWrXlpW--"
      },
      "source": [
        "First, let's check if we have missing values for the customer ID column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dEoOzT3pWS2"
      },
      "source": [
        "online_retail[online_retail['CustomerID'].isna()] # computing the dataframe af all the rows where the customer ID is missing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asCOXiC0px5e"
      },
      "source": [
        "So, for our data analysis, 135 080 rows will be useless, since they do not have any customerID! Let's get rid of those:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNvEdofDo6jv"
      },
      "source": [
        "online_retail = online_retail[~online_retail['CustomerID'].isna()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZnGmAUo14th"
      },
      "source": [
        "Now, let's build a list of unique CustomerID:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWBW_Dp1WmsH"
      },
      "source": [
        "list_of_customers = online_retail['CustomerID'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJVkHU1_2Nwt"
      },
      "source": [
        "We can perform the train-test split on this data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ezvNVsyqf5n"
      },
      "source": [
        " train_customers, test_customers = train_test_split(list_of_customers, # original dataframe to be tested\n",
        "                                    test_size=0.2, # proportion of the rows to put in the test set\n",
        "                                    random_state=42) # for reproducibility (see explanation below)\n",
        "\n",
        "# note: we randomly split the dataset into train and test. However, we want to \n",
        "#       make sure that the split will be the same whenever we run this notebook.\n",
        "#       for this, we use the random_state argument (we could choose any number,\n",
        "#       we just need to make sure that we don't change it, and the split will\n",
        "#       always be the same, no matter how many times we open and close this\n",
        "#       notebook)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8SA304uj_YR"
      },
      "source": [
        "Now that we have the customer IDs seperated in a train and test set, let's create our train and test datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKi6ZY4mW9Mr"
      },
      "source": [
        "train = online_retail[online_retail['CustomerID'].isin(train_customers)].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6f0DnZvkc7i"
      },
      "source": [
        "test = online_retail[online_retail['CustomerID'].isin(test_customers)].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOJl4j2kxSrl"
      },
      "source": [
        "from now on, we will use the train dataframe. let's save the train and test datasets in the data folder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcoPyMRyktCx"
      },
      "source": [
        "train.to_csv(f'{data_path}OnlineRetailTrain.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ8E75zzk3LK"
      },
      "source": [
        "test.to_csv(f'{data_path}OnlineRetailTest.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLW5UTSz2cNX"
      },
      "source": [
        "**perform an automatic profiling of the train dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lopzsEgB3yG"
      },
      "source": [
        "profile = ProfileReport(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wCiTnA2CAjr"
      },
      "source": [
        "profile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn-Y7pq0OIZ1"
      },
      "source": [
        "### Missing Values and Duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAXs60uBP5v5"
      },
      "source": [
        "**Are there some duplicate columns? rows?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BhLYYGVTtIw"
      },
      "source": [
        "Let's print again the columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aJ2D7L3TzA-"
      },
      "source": [
        "train.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXW7sHKxxaQa"
      },
      "source": [
        "and the first rows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb67u8B_T5da"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0UMx_DRT853"
      },
      "source": [
        "Every column has a specific meaning, there are no duplicate columns.\n",
        "\n",
        "Let's check for duplicate rows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SO5J8XhNQI6y"
      },
      "source": [
        "train.duplicated().value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaBFPt8CUYw2"
      },
      "source": [
        "it seems that we have 4000 duplicate rows. Let's make a summary data frame counting all of them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDJr2wvOQgeR"
      },
      "source": [
        "duplicates = (train[train.duplicated(keep=False)] # select all the duplicated rows\n",
        "              .groupby(by=list(train.columns)) # group matching rows\n",
        "              .size() # counts how many duplicates for each specific row value\n",
        "              .reset_index() # reshape the result as a DataFrame\n",
        "              .rename(columns={0:'DuplicatesNumber'}) # rename the duplicates number colums as 'DuplicatesNumber'\n",
        "              ) \n",
        "duplicates.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ConjFOuhhnm"
      },
      "source": [
        "Now, we can extract some more information on the number of duplicates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5ju5Fo2VQxi"
      },
      "source": [
        "duplicates['DuplicatesNumber'].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbKVUhTXhhC7"
      },
      "source": [
        "We have 3694 rows that are duplicated, most of them appear just twice. At this point, we should contact the data collecting team to clarify whether we should drop them or keep them in the data frame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn06rLlNiyH_"
      },
      "source": [
        "**Should we drop duplicate rows?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLT3pnaKiftK"
      },
      "source": [
        "let's suppose we want to drop all the duplicates:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCs3F8kHikn0"
      },
      "source": [
        "train.drop_duplicates(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCobMj8jjuKW"
      },
      "source": [
        "train.duplicated(keep=False).value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qCqRb45i0iY"
      },
      "source": [
        "**How many missing values are there in each columns?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1otW6VQWllkZ"
      },
      "source": [
        "to answer this question, we can use the count method, which counts the number of rows that have a value (the non-missing data!): "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTaFoeN0mHq7"
      },
      "source": [
        "train.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBqDKdeUmc13"
      },
      "source": [
        "we can see that there are no data missing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REXLqfbf3Gdd"
      },
      "source": [
        "### Data Distribution and Outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJM0AMKFnBfw"
      },
      "source": [
        "**What is the distribution of numerical/categorical data?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0eFLGoACpQU"
      },
      "source": [
        "train.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5KC2_beRfTV"
      },
      "source": [
        "Let's plot some plotly histograms and boxplots for our numerical variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "If7ZbtxuOF-J"
      },
      "source": [
        "fig = px.histogram(train, x=\"Quantity\", nbins=2000)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAfKWcr1P-60"
      },
      "source": [
        "fig = px.box(train, x=\"Quantity\")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM3DXQYORo72"
      },
      "source": [
        "as we saw before, there are some outliers: with plotly, we can zoom in and get more information on them directly from the graph!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRBrgsLOOifK"
      },
      "source": [
        "fig = px.histogram(train, x=\"UnitPrice\", nbins=2000)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqEGXozDPjqA"
      },
      "source": [
        "fig = px.box(train, x=\"UnitPrice\")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TECcqKpnR70C"
      },
      "source": [
        "Likewise, for categorical variables, it is much easier to examine small categories by zooming in and hovering onto the bars:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTJyeNDP4B_l"
      },
      "source": [
        "fig = px.histogram(train, x=\"Country\")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB0DgmVSnF6T"
      },
      "source": [
        "**Are there clear outliers?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_PmP7D-40xC"
      },
      "source": [
        "it seems indeed that some quantity order are far away from the distribution. let's investigate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0PwxteB5LFP"
      },
      "source": [
        "train[(train['Quantity']>5000) | (train['Quantity']<-5000)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7YXFkPP5zvD"
      },
      "source": [
        "These outliers correspond to orders that have been almost instantly cancelled (541431), or with a unit price of 0. We might have to remove them!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-kK2zNs6YZX"
      },
      "source": [
        "let's investigate the UnitPrices> 2000:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d06mFlAK6Ys1"
      },
      "source": [
        "train[train['UnitPrice']>2000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HidWZ8Zs7vSu"
      },
      "source": [
        "likewise, most of the outlier orders seem to have been quickly cancelled."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GehmrkNYnKGt"
      },
      "source": [
        "**Can we rule out some outliers as mistakes in the data collecting process?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ijZQyRp7FJx"
      },
      "source": [
        "It seems that the outliers are mistakes from the customer side."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-fxxlW-nNjs"
      },
      "source": [
        "**How should we deal with outliers?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTc5FUbe7P0z"
      },
      "source": [
        "We might want to get rid of them during our segmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDjZxsk9nUZD"
      },
      "source": [
        "### Relationship between features (correlations)\n",
        "\n",
        "**What are the relationships between features (make a pairplot)? Are they linear?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8suLPkl8MJO"
      },
      "source": [
        "we do not have many numeric columns here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voHLy_vRLnnQ"
      },
      "source": [
        "train.corr().style.background_gradient(cmap='coolwarm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYnNvkPsSOzI"
      },
      "source": [
        "Let's try a 2D density plot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUx04nQ_QuuH"
      },
      "source": [
        "fig = px.density_heatmap(train, x=\"Quantity\", y=\"UnitPrice\", marginal_x=\"histogram\", marginal_y=\"histogram\",nbinsx=2000, nbinsy=2000)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alBVnaTcS1vE"
      },
      "source": [
        "not very promising... let's make a scatter plot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_o6vXu5Q3W5"
      },
      "source": [
        "fig = px.scatter(train, x=\"Quantity\", y=\"UnitPrice\")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeD8SXpu8UUV"
      },
      "source": [
        "we can see that the distribution of quantity and price is in the shape of a T: the most ordered items are low price, and the expensive items are ordered just one by one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S79ZQwy-ndzg"
      },
      "source": [
        "**What correlation coefficients should be computed?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuxwlpTfD1rl"
      },
      "source": [
        "Computing correlation coefficients will not really help here, since the variables do not have any kind of linear relationships"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsrsvOwlniij"
      },
      "source": [
        "**Is there risk of data leakage?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giqQ2aeAEIvv"
      },
      "source": [
        "None identified so far."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBmvL4ZMIg7p"
      },
      "source": [
        "### Feature Creation and Combination\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoSV_oWW3IGH"
      },
      "source": [
        "Now that we have worked some time on exploring the dataset, it is time to think again about the original problem we want to solve:\n",
        "\n",
        "**_Can we segment our Customer base in different meaningful groups?_**\n",
        "\n",
        "Let's recap the questions to consider for Feature Creation and Combination:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSY3Um3-M4CG"
      },
      "source": [
        "- **What kind of Scaling should we use/try?**\n",
        "- **Should we transform some features?**\n",
        "- **Should we drop some features?**\n",
        "- **Should we combine features?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCLA7tupwyUL"
      },
      "source": [
        "A classical way of looking at customer behavior is to compute 3 variables of interest for businesses:\n",
        "\n",
        "- **Recency:** days since the last purchase of a given customer\n",
        "- **Frequency:** the total number of orders a given customer made\n",
        "- **Monetary Value:** the total amount of money a given customer spent.\n",
        "\n",
        "This is called the RFM model, and we will cluster our customer data on these criteria. Let's compute those."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZz79LmsU89j"
      },
      "source": [
        "to compute the monetary value, we will need the total spent for each row. let's create the column in the train DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt-FzsAADgxS"
      },
      "source": [
        "train['Total'] = train['Quantity']*train['UnitPrice']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3NTsAVuVTOA"
      },
      "source": [
        "As highlighted before, our approach is centered on customers. We can start by grouping the data by customerID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0HgFXwQ9SXe"
      },
      "source": [
        "data_by_customer = train.groupby('CustomerID')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA5f2e0LVVmb"
      },
      "source": [
        "Let's compute the recency with respect to the last date in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBNI99TPCiAP"
      },
      "source": [
        "present = train['InvoiceDate'].max()\n",
        "recency = pd.DataFrame((present - data_by_customer['InvoiceDate'].max()).dt.days)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwDWbMAYVfHw"
      },
      "source": [
        "let's now compute frequency and monetary values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcg6Ovy8DCcH"
      },
      "source": [
        "frequency = pd.DataFrame(data_by_customer.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRJQXUDTDOOQ"
      },
      "source": [
        "monetary_value = pd.DataFrame(data_by_customer['Total'].sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MliWw6ByVkKg"
      },
      "source": [
        "finally, we put everything together in a DataFrame called RFM. to do so, we use the merge method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtuxgaZjJYw1"
      },
      "source": [
        "RFM = recency"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jV0TwJfPEEu9"
      },
      "source": [
        "RFM = RFM.merge(frequency , left_index = True, right_index=True )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG90k5XaI_c-"
      },
      "source": [
        "RFM = RFM.merge(monetary_value , left_index = True, right_index=True )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79GH22_rH5Yj"
      },
      "source": [
        "RFM.columns = ['recency', 'frequency', 'monetary_value']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxjZxDMZJF-8"
      },
      "source": [
        "RFM.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH2PBOP5TxPY"
      },
      "source": [
        "Let's plot the distribution of each value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKBgcNeuUFiC"
      },
      "source": [
        "for column in RFM:\n",
        "    fig = px.histogram(RFM, x=column)\n",
        "    fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ob3wJq4UYBa"
      },
      "source": [
        "We can see there are some outliers in the recency and monetary_value. We can get rid of these by using the Interquartile range score method: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkb0NEBjW1Df"
      },
      "source": [
        "Q1 = RFM.quantile(0.25)\n",
        "Q3 = RFM.quantile(0.75)\n",
        "IQR = Q3-Q1\n",
        "print(IQR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7te3Gg0hYIf4"
      },
      "source": [
        "with this method, we have to exclude the rows for each at least one of the values is outside `(Q1 - 1.5 IQR), (Q3 + 1.5 IQR)`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1NMpcc3YkEr"
      },
      "source": [
        "is_outlier = (RFM < (Q1 - 1.5 * IQR)) |(RFM > (Q3 + 1.5 * IQR))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25VaK3oeZVYB"
      },
      "source": [
        "RFM_no_outliers = RFM[~(is_outlier['recency']|is_outlier['frequency']|is_outlier['monetary_value'])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFK764DiY_cN"
      },
      "source": [
        "RFM_no_outliers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrQdEleRJi7D"
      },
      "source": [
        "fig = px.scatter_3d(RFM_no_outliers, x='recency', y='frequency', z='monetary_value')\n",
        "fig.update_traces(marker=dict(size=2))\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FVIOLGLgGbD"
      },
      "source": [
        "for completeness, let's plot the RFM with the outliers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuEN2pgyaYFy"
      },
      "source": [
        "fig = px.scatter_3d(RFM, x='recency', y='frequency', z='monetary_value')\n",
        "fig.update_traces(marker=dict(size=2))\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7bdAEkvjYnb"
      },
      "source": [
        "Now that we have our features, let's tale a look at the correlation matrix. Let's compare the dataset with outliers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhcUOlCDa8_Y"
      },
      "source": [
        "RFM.corr().style.background_gradient(cmap='coolwarm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZwuQ00OnrvV"
      },
      "source": [
        "with the dataset without outliers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o13WWtbxnhkl"
      },
      "source": [
        "RFM_no_outliers.corr().style.background_gradient(cmap='coolwarm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NfY99uujlfX"
      },
      "source": [
        "We can see that frequency and monetary values have a high correlation, which makes sense (the more frequent customers are, the more they are likely to bring high revenue to the business). Recency is negatively correlated to frequency and monetary values: customers who purchased recently might be more likely to be new customers, so have a lower monetary value and lower frequency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHLET0Xsa8_l"
      },
      "source": [
        "Let's try a 2D density plot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFF5o9Qaa8_l"
      },
      "source": [
        "fig = px.density_heatmap(RFM_no_outliers, x=\"recency\", y=\"frequency\", marginal_x=\"histogram\", marginal_y=\"histogram\",nbinsx=20, nbinsy=20)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3ZdDLfWnl6C"
      },
      "source": [
        "### Conclusion: Experimental setup and  Possible Feature Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFBivdWZnqdy"
      },
      "source": [
        "Let's wrap up on the exploratory data analysis and conclude. We should now be able to answer the following questions:\n",
        "\n",
        "- **What would be our baseline for the analysis?**\n",
        "- **What kind of modelling setup should we use/try?**\n",
        "- **What kind of Scaling should we use/try?**\n",
        "- **If outliers, what kind of treatment should we apply?**\n",
        "- **Should we transform some features?**\n",
        "- **Should we drop some features?**\n",
        "- **Should we combine features?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKg_1CqQduN-"
      },
      "source": [
        "For our problem, we should use a clustering technique, to be able to segment our customer base into meaningful groups.\n",
        "\n",
        "We introduced some domain knowledge with the RFM (Recency-Frequency-Monetary Value) model. Those are the features we should base our analysis on.\n",
        "\n",
        "We have seen that there are some outliers and we could test fitting a clustering model to the data without the outliers.\n",
        "\n",
        "To be able to cluster our customers, it is important to have features which vary over the same range, so that our clustering algorithm take into account each feature with equal importance. We should rescale our features with a standard scaler for example.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lobDaWXNAXRz"
      },
      "source": [
        "<a id='P2' name=\"P2\"></a>\n",
        "## [Modelling](#P0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ltofBG-RaKU"
      },
      "source": [
        "### Base Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwPZKKnFbbCT"
      },
      "source": [
        "#### Pipeline Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YADbsIjWbn54"
      },
      "source": [
        "scale = StandardScaler()\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "kmeans = KMeans(n_clusters=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PezQvW0be55"
      },
      "source": [
        "clustering_model = Pipeline(steps=[('preprocessor', scale),\n",
        "                                   ('imputer', imputer)\n",
        "                                   ('clustering', kmeans)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubuG34ojbi3V"
      },
      "source": [
        "#### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzqRq3hkM4-7"
      },
      "source": [
        "cross_val_score(clustering_model, RFM_no_outliers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPdaQIaIblAA"
      },
      "source": [
        "clustering_model.fit(RFM_no_outliers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_FAAZs9PeId"
      },
      "source": [
        "visualizer = SilhouetteVisualizer(clustering_model['clustering'], colors='yellowbrick')\n",
        "visualizer.fit(clustering_model['preprocessor'].transform(RFM_no_outliers))\n",
        "visualizer.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pekHQD71Rkke"
      },
      "source": [
        "### Model Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36q3GzdqRnzi"
      },
      "source": [
        "When clustering, we can test the different number of clusters for a given model using yellowbrick KelbowVisualizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MMcjNKCQ-a8"
      },
      "source": [
        "visualizer = KElbowVisualizer(clustering_model['clustering'], k=(2,12))\n",
        "\n",
        "visualizer.fit(clustering_model['preprocessor'].transform(RFM_no_outliers)) \n",
        "visualizer.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKNNpBOyR6kL"
      },
      "source": [
        "Here, the method would suggest 4 clusters instead of 3 for example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQUOjdTrAj8Y"
      },
      "source": [
        "<a id='P3' name=\"P3\"></a>\n",
        "## [Model Evaluation](#P0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ur49TJJhpGV5"
      },
      "source": [
        "test['Total'] = test['Quantity']*test['UnitPrice']\n",
        "data_by_customer = test.groupby('CustomerID')\n",
        "recency = pd.DataFrame((present - data_by_customer['InvoiceDate'].max()).dt.days)\n",
        "frequency = pd.DataFrame(data_by_customer.size())\n",
        "monetary_value = pd.DataFrame(data_by_customer['Total'].sum())\n",
        "RFM_test = recency\n",
        "RFM_test = RFM_test.merge(frequency , left_index = True, right_index=True )\n",
        "RFM_test = RFM_test.merge(monetary_value , left_index = True, right_index=True )\n",
        "RFM_test.columns = ['recency', 'frequency', 'monetary_value']\n",
        "RFM_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV_ud_VPqHkJ"
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPaudjEvpGWg"
      },
      "source": [
        "Let's plot the distribution of each value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvCBudBWpGWh"
      },
      "source": [
        "for column in RFM_test:\n",
        "    fig = px.histogram(RFM_test, x=column)\n",
        "    fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn3KUtdDGUh3"
      },
      "source": [
        "clusters = clustering_model.predict(RFM_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfhSQ0_yG6wH"
      },
      "source": [
        "fig = px.scatter_3d(RFM_test, x='recency', y='frequency', z='monetary_value', color=clusters)\n",
        "fig.update_traces(marker=dict(size=2))\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxMjuHW7AmRQ"
      },
      "source": [
        "silhouette_score(RFM_test, clusters)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}